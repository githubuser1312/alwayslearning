MODELOS LINEALES--------------

Estos modelos utilizan una fórmula simple para encontrar la línea que mejor se ajuste a traves de puntos de datos y así poder predecir valores futuros.

Los modelos lineales son considerados la "vieja escuela" (a menudo no son tan predictivos como los nuevos algoritmos, pero son más rápidos y sencillos).

----------------------REGRESION LINEAL SIMPLE-----------------------------
Predice una respuesta usando una sóla característica. Se supone que las dos variables (la independiente y la dependiente) tienen una relación lineal.
x = [x_1,......,x_n] es el vector de características y y = [y_1,......,y_n] es el vector de respuestas para n observaciones. Los datos se pueden representar en un gráfico de dispersión y la tarea es la de encontrar un línea que se ajuste lo mejor posible al diagrama de dispersion para que podamos predecir una respuesta para cualquier valor nuevo. Esa línea se llama LINEA DE REGRESION. 
La ecuación de la linea de regresión es:

                    y = a + bx

donde y es la respuesta prevista para la x observación, a representa la intersección y b la pendiente de la linea de regresión. Para crear el modelo tenemos que 'aprender' o estimar los valores de a y b. Despúes podremos usar el modelo para predecir valores.

En el ejemplo que aparece en 06Ejemplos, se usa la fórmula anterior pero incluyendo un e (error) que debe ser el más pequeño posible 

                    y = a + bx + e

para ello habrá que encontrar a y b que hagan que ese error sea el menor posible.

Ver ejemplo en en el archivo 06Ejemplos.py

----------------------REGRESION LINEAL MULTIPLE-----------------------------
Este tipo de regresión intenta modelar la relación entre dos o más características y una respuesta ajustando una ecucación lineal a los datos observados (es una extensión de la regresión lineal).
En este caso consideramos un conjunto de datos con p características ( o variables independientes) y una respuesta (o variable dependiente). El conjunto de datos contiene n filas/observaciones.

Sea X (matriz de características) = una matriz de tamaño n x p donde x_{ij} denota los valores de la característica j-ésima para la observación i-ésima.

Ver ejemplo en en el archivo 06Ejemplos.py

----------------------REGRESION LOGISTICA-----------------------------
La clasificación es un área muy importante del aprendizaje automático supervisado. Un gran número de problemas importantes de aprendizaje automático caen dentro de este área. Hay muchos métodos de clasificación, y la regresión logística es uno de ellos.

Se puede aplicar la clasificación en muchos campos de la ciencia y la tecnología. Por ejemplo, los algoritmos de clasificación de texto se utilizan para separar los emails legítimos y no deseados, así como los comenterios positivos y negativos. Otros ejemplos involucran aplicaciones médicas, clasificación biologica, calificación crediticia, etc.

Las tareas de reconocimiento de imágenes a menudo se representan como problemas de clasificación. Por ejemplo, se puede preguntar si una imagen representa un rostro humano o no, o si es un ratón o un elefante, o que dígito de cero a nueve representa, y así sucesivamente.

La regresión logística es una técnica de clasificación fundamental. Pertenece al grupo de los clasificadores lineales y es algo similar a la regresión polinómica y lineal. La regresión logística es rápida y relativamente sencilla, y es necesario interpretar los resultados. Aunque es esencialmente un método para la clasificación binaria, también se puede aplicar a problemas multiclase.

Para comprender la regresión logística necesitamos comprender la función SIGMOIDE ( https://es.wikipedia.org/wiki/Funci%C3%B3n_sigmoide )y la función LOGARITMO NATURAL ( https://es.wikipedia.org/wiki/Logaritmo_natural ).

La FUNCION SIGMOIDE tiene valores muy cercanos a 0 o 1 en la mayor parte de su dominio. Este hecho lo hace adecuado para su aplicación en métodos de clasificación.

La función logaritmo natural se representa como ln, sin embargo en Python: math.log(x) y np.log(x) representan el logaritmo natural de x.

Veamos la regresión logística aplicada a la clasificación binaria. Cuando se implementa la regresión logística de alguna variable dependiente y en el conjunto de variables independientes x = (x1, ....., xr), donde r es el número de predictores (o entradas), se comienza con los valores conocidos de los predictores xi y la correspondiente respuesta real (o salida) yi para cada observación i = 1, .... ,n.
La meta es encontrar la función de regresión logística p(x) tal que las respuestas predichas p(xi) estén lo más cerca posible de la respuesta real yi para cada observación i = 1, ...... ,n.
-------------------------------------------------------------------------------------------
IMPORTANTE: LA RESPUESTA REAL SÓLO PUEDE SER 0 o 1 EN PROBLEMAS DE CLASIFICACION BINARIA. ESTO SIGNIFICA QUE CADA p(xi) DEBE ESTAR CERCA DE 0 o 1. Por eso es conveniente utilizar la función SIGMOIDE.
-------------------------------------------------------------------------------------------

Una vez que tengamos la función de regresión logística p(x), podemos usarla para predecir las salidas de nuevas entradas, asumiendo que la dependencia matemática subyacente no ha cambiado.

La regresión logística es un CLASIFICADOR LINEAL por lo que se usa una función lineal:

        f(x) = b0 + b1x1 + ........ + brxr 

también llamada LOGIT. 

Las variables b0, b1, ...... br son los estimadores de los coeficientes de regresión, que también se denominan PESOS PREDICHOS o simplemente coeficientes.

La función de regresión logística p(x) es la función sigmoide de f(x):

                p(x) = 1 / (1 + exp(-f(x)))

Como tal, normalmente está cerca de 0 o de 1. La función p(x) a menudo se interpreta como la probabilidad predicha de que la salida para una x dada sea igual a 1. por lo tanto, 1-p(x) es la probabilidad de que la salida sea 0.

La regresión logística determina los mejores PESOS PREDICHOS b0, ..... br tal que la función p(x) es lo más cercana posible a todas las respuestas reales yi, i = 1,....,n donde n es el número de observaciones. El proceso de calcular los mejores PESOS utilizando las observaciones disponibles se denomina ENTRENAMIENTO o AJUSTE DEL MODELO.

Para obtener los mejores PESOS, generalmente se maximiza la función de probabilidad logarítmica (LLF - Log-likelihood Function) para todas las observaciones i = 1,..., n. Este método se denomina estimación de máxima verosimilitud y está representado por la ecuación:

                LLF = ∑i(yi log(p(xi))+ (1 -yi) log(1 - p(xi)))

Cuando yi = 0, el LLF para la observacion correspondiente es igual a log(1-p(xi)). Si p(xi) está cerca de yi = 0, entonces log(1-p(xi)) está cerca de 0. Ese es el resultado que queremos.

Si p(xi) está lejos de 0, entonces log(1-p(xi)) cae significativamente. No queremos ese resultado porque el objetivo es obtener el máximo LLF.

Del mismo modo cuando yi = 1, el LLF para esa observación es yilog(p(xi)). Si p(xi) está cerca de yi = 1, entonces log(p(xi)) está cerca de 0. Si p(xi) está lejos de 1, entonces log(p(xi)) es un número negativo grande.

Hay varios enfoques matemáticos que calcularán los mejores pesos que corresponden al LLF máximo, pero las bibliotecas Python de regresión logística los calcularán por nosotros.

Una vez determinados los mejores pesos que definen la función p(x), podemos obtener las salidas predichas p(xi) para cualquier entrada dada xi. Para cada observación i = 1,..., n, la salida predicha es 1 si p(xi) > 0,5 y 0 en caso contrario. El umbral no tiene que ser 0,5, pero generalmente lo es. Es posible definir un valor más bajo o más alto si se considera necesario.

Hay una relación más importante entre p(x) y f(x), que es que log(p(x)/(1-p(x))) = f(x). esta igualdad explica por que f(x) es el LOGIT. Implica que p(x) = 0.5 cuando f(x)= 0 y que la salida predicha es 1 si f(x) >0 y 0 en caso contrario.

Para medir el rendimiento de la clasificación binaria, tenemos 4 tipos posibles de resultados:

        * Verdaderos negativos (ceros predichos correctamente)
        * Verdaderos positivos (unos predichos correctamente)
        * Falsos negativos (ceros predichos incorrectamente)
        * Falsos positivos (unos predichos incorrectamente)

Indicadores de clasificadores binarios pueden ser los siguientes:

                    * PRECISION  (explicado en cap. anerior)
                    * VALOR PREDICTIVO POSITIVO (verdaderos positivos / (verdaderos positivos+falsos positivos)).
                    * VALOR PREDICTIVO NEGATIVO (verdaderos negativos / (verdaderos negativosfalsos negativos)).
                    * SENSIBILIDAD / RECUERDO / TASA POSITIVA VERDADERA (es la relación entre el número de verdaderos positivos y el número de positivos reales).
                    * ESPECIFIDAD / TASA NEGATIVA VERDADERA (es la relación entre el número de negativos verdaderos y el número de negativos reales).



----------------------SUPOSICIONES SOBRE EL CONJUNTO DE DATOS-----------------------------
A continuación revisamos los supuestos básicos que un modelo de regresión lineal hace con respecto a un conjunto de datos en el que se aplica:

        * Relación Lineal: La relación entre las variables de respuesta y de característica debe ser lineal. La suposicion de linealidad se puede probar utilizando diagramas de dispersión. 

        * Poca o ninguna Multicolinealidad: se supone que hay poca o ninguna multicolinealidad en los datos. La multicolinealidad ocurre cuando las características (o variables independientes) no son independientes entre sí.

        * Homocedasticidad: describe una situación en la que el termino error (ruido o perturbación aleatoria en la relacion entre las variables independientes y la variable dependiente) es el mismo en todos los valores de las variables independientes. 